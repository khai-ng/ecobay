services:
    ##########################################
    ## Kafka
    ##########################################
    kafka-ui:
        profiles: [ kafka, all ]
        container_name: kafka-ui
        image: provectuslabs/kafka-ui:latest
        depends_on:
            kafka:
                condition: service_healthy
            zookeeper:
                condition: service_healthy
        networks:
            - local 

    zookeeper:
        profiles: [ kafka, minimal-workflow, all ]
        container_name: zookeeper
        image: confluentinc/cp-zookeeper:7.6.0
        healthcheck:
            test: echo ruok | nc 127.0.0.1 2181 || exit -1
            interval: 10s
            timeout: 10s
            retries: 3
        networks:
            - local 
 
    kafka:
        profiles: [ kafka, minimal-workflow, all ]
        container_name: kafka
        image: confluentinc/cp-kafka:7.6.0
        healthcheck:
            test: nc -z localhost 9092 || exit -1
            interval: 10s
            timeout: 5s
            retries: 5
        depends_on:
            zookeeper:
                condition: service_healthy
        networks:
            - local 

    # schemaregistry:
    #     container_name: schemaregistry
    #     image: confluentinc/cp-schema-registry:7.6.0
    #     depends_on:
    #         - kafka
    #     networks:
    #         - local 

    # kafka-connect:
    #     container_name: kafka-connect
    #     image: confluentinc/cp-kafka-connect:7.6.0
    #     depends_on:
    #         - kafka
    #         - schemaregistry
    #     networks:
    #         - local 

    ##########################################
    ## Services
    ##########################################
    web-apigateway:
        profiles: [ services, minimal-workflow, all ]
        container_name: web-apigateway
        image: ${DOCKER_REGISTRY-}webapigateway
        build:
            context: .
            dockerfile: src/Services/ApiGateways/Web.ApiGateway/Dockerfile
        # depends_on:
        #     - identity-api
        networks:
            - local
        logging:
            driver: fluentd
            options:
                fluentd-async: "true"
                tag: host.web-apigateway  

    pg-eventstore-db:
        profiles: [ events-store, minimal-workflow, services, all ]
        container_name: pg-eventstore-db
        image: postgres:15-alpine
        healthcheck:
            test: pg_isready -U postgres -d postgres
            interval: 10s
            timeout: 3s
            retries: 3
        networks:
            - local
    
    # Ordering
    ordering-api:
        profiles: [ events-store, minimal-workflow, services, all ]
        container_name: ordering-api
        image: ${DOCKER_REGISTRY-}orderingapi
        build:
              context: .
              dockerfile: src/Services/Ordering/Ordering.API/Dockerfile
        healthcheck:
            test: curl --fail https://ordering-api/hc || exit 1
            interval: 30s
            timeout: 5s
            retries: 3
        networks:
            - local
        logging:
            driver: fluentd
            options:
                fluentd-async: "true"
                tag: host.ordering

    # Identity
    # identity-api:
    #     container_name: identity-api
    #     image: ${DOCKER_REGISTRY-}identityapi
    #     build:
    #         context: .
    #         dockerfile: src/Services/Identity/Identity.API/Dockerfile
    #     # depends_on:
    #     #     kafka:
    #     #         condition: service_healthy
    #     networks:
    #         - local
    #     logging:
    #         driver: fluentd
    #         options:
    #             fluentd-async: "true"
    #             tag: host.identity
    
    keycloak:
        profiles: [ services, minimal-workflow, all ]
        container_name: keycloak
        image: quay.io/keycloak/keycloak:latest
        restart: on-failure
        networks:
            - local

    # mysql db. using service per schema
    mysql-db:
        profiles: [ services, minimal-workflow, all ]
        container_name: mysql-db
        image: mysql:8.0.34
        healthcheck:
            test: ["CMD", "mysqladmin" ,"ping", "-h", "localhost"]
            timeout: 20s
            retries: 5
        restart: on-failure
        networks:
            - local

    # Product
    product-api-1:
        profiles: [ services, minimal-workflow, all ]
        container_name: product-api-1
        image: ${DOCKER_REGISTRY-}productapi
        build:
            context: .
            dockerfile: src/Services/Product/Product.API/Dockerfile
        healthcheck:
            test: curl --fail https://product-api/hc || exit 1
            interval: 30s
            timeout: 5s
            retries: 3
        networks:
            - local
        logging:
            driver: fluentd
            options:
                fluentd-async: "true"
                tag: host.product-1

    router01:
        profiles: [ mongo-shard ]
        container_name: product-router01
        image: mongo:7.0.9
        command: mongos --port 27017 --configdb config-server/configsvr01:27017,configsvr02:27017 --bind_ip_all
        ports:
            - 27117:27017
        volumes:
            - ./configs/mongodb:/scripts
        networks:
            - local

    configsvr01:
        profiles: [ mongo-shard ]
        container_name: product-config01
        image: mongo:7.0.9
        command: mongod --port 27017 --configsvr --replSet config-server
        volumes:
            - ./configs/mongodb:/scripts 
        ports:
            - 27119:27017
        links:
            - shard01-1
            - shard02-1
        networks:
            - local

    configsvr02:
        profiles: [ mongo-shard ]
        container_name: product-config02
        image: mongo:7.0.9
        command: mongod --port 27017 --configsvr --replSet config-server
        volumes:
            - ./configs/mongodb:/scripts 
        ports:
            - 27120:27017
        links:
            - configsvr01
        networks:
            - local

    shard01-1:
        profiles: [ mongo-shard ]
        container_name: product-shard01
        image: mongo:7.0.9
        command: mongod --replSet shard-01 --shardsvr --port 27017
        volumes:
            - ./configs/mongodb:/scripts
        ports:
            - 27020:27017
        links:
            - shard01-2
        networks:
            - local

    shard01-2:
        profiles: [ mongo-shard ]
        container_name: product-shard01-1
        image: mongo:7.0.9
        command: mongod --replSet shard-01 --shardsvr --port 27017
        volumes:
            - ./configs/mongodb:/scripts
        ports:
            - 27021:27017
        networks:
            - local

    shard02-1:
        profiles: [ mongo-shard ]
        container_name: product-shard02
        image: mongo:7.0.9
        command: mongod --replSet shard-02 --shardsvr --port 27017
        volumes:
            - ./configs/mongodb:/scripts
        ports:
            - 27022:27017
        links:
            - shard02-2
        networks:
            - local

    shard02-2:
        profiles: [ mongo-shard ]
        container_name: product-shard02-1
        image: mongo:7.0.9
        command: mongod --replSet shard-02 --shardsvr --port 27017
        volumes:
            - ./configs/mongodb:/scripts
        ports:
            - 27023:27017
        networks:
            - local
    ##########################################
    ## Monitoring
    ##########################################
    fluent-bit:
        profiles: [ logging, all ]
        container_name: fluent-bit
        image: fluent/fluent-bit:2.2.2
        networks:
            - local

    ## Fluent bit - Elasticsearch, Kibana
    # elasticsearch:
    #     container_name: elasticsearch
    #     image: docker.elastic.co/elasticsearch/elasticsearch:7.17.18
    #     restart: on-failure
    #     networks:
    #         - local
    #     depends_on:
    #         - fluent-bit

    # kibana:
    #     container_name: kibana
    #     image: docker.elastic.co/kibana/kibana:7.17.18
    #     restart: on-failure
    #     networks:
    #         - local
    #     depends_on:
    #         - fluent-bit
    #         - elasticsearch

    # Fluent bit - Loki, Grafana
    loki:
        profiles: [ logging, all ]
        container_name: loki
        image: grafana/loki:2.9.4
        networks:
            - local
    
    grafana:
        profiles: [ logging, obverser, all ]
        container_name: grafana
        image: grafana/grafana:10.2.4
        networks:
            - local
        depends_on:
            - fluent-bit
            - loki

    # Observer
    prometheus:
        profiles: [ obverser, all ]
        container_name: prometheus
        image: prom/prometheus:v2.49.1
        restart: unless-stopped
        networks:
            - local
        labels:
            org.label-schema.group: "monitoring"

    nodeexporter:
        profiles: [ obverser, all ]
        container_name: nodeexporter
        image: prom/node-exporter:v1.7.0
        restart: unless-stopped
        networks:
            - local
        labels:
            org.label-schema.group: "monitoring"

    cadvisor:
        profiles: [ obverser, all ]
        container_name: cadvisor
        image: gcr.io/google-containers/cadvisor:v0.36.0
        restart: unless-stopped
        networks:
            - local
        labels:
            org.label-schema.group: "monitoring"

    # alertmanager:
    #     container_name: alertmanager
    #     image: prom/alertmanager:v0.26.0
    #     restart: unless-stopped
    #     networks:
    #         - local
    #     labels:
    #         org.label-schema.group: "monitoring"

    jaeger:
        profiles: [ tracing, all ]
        image: jaegertracing/all-in-one:latest
        container_name: jaeger
        networks:
            - local
networks:
    local:
        driver: bridge
    # kafka:
    #     driver: bridge
    # monitoring:
    #     driver: bridge
